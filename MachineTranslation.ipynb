{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MachineTranslation.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyND8mwtnU3xrYEh4b45zeRd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AmanPriyanshu/Natural-Language-Processing/blob/master/MachineTranslation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4wL_3LAtMCV",
        "colab_type": "text"
      },
      "source": [
        "## IMPORTS:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "89M0lS85nQpf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "import unicodedata\n",
        "import re\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import string\n",
        "from tqdm import tqdm\n",
        "from sklearn.neighbors import KNeighborsClassifier"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjMbKCAftNxJ",
        "colab_type": "text"
      },
      "source": [
        "## GETTING DATA:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEr3je7RnLA3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "3876d840-bbdb-4fc0-cf02-b778d1701125"
      },
      "source": [
        "path_to_zip = tf.keras.utils.get_file(\n",
        "    'fra-eng.zip', origin='http://download.tensorflow.org/data/fra-eng.zip', \n",
        "    extract=True)\n",
        "\n",
        "path_to_file = os.path.dirname(path_to_zip)+\"/fra.txt\""
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from http://download.tensorflow.org/data/fra-eng.zip\n",
            "3424256/3423204 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VV1_sTZQtPpS",
        "colab_type": "text"
      },
      "source": [
        "## PREPROCESSING:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zkxY9_nsnfNH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def unicode_to_ascii(s):\n",
        "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn')\n",
        "    \n",
        "def preprocess_sentence(w):\n",
        "    w = unicode_to_ascii(w.lower().strip())\n",
        "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
        "    w = re.sub(r'[\" \"]+', \" \", w)\n",
        "    w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
        "    w = w.rstrip().strip()\n",
        "    w = ' '.join([i for i in w.split() if i not in string.punctuation])\n",
        "    w = '<start> ' + w + ' <end>' \n",
        "    return w"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Si_X7rMtSBL",
        "colab_type": "text"
      },
      "source": [
        "## GENERATING DATASET:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sSod1HJnoRpe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_dataset(path, num_examples):\n",
        "    lines = open(path, encoding='UTF-8').read().strip().split('\\n')\n",
        "    \n",
        "    word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')]  for l in lines[:num_examples]]\n",
        "    \n",
        "    return word_pairs"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1-Q3jmsonRa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pairs = create_dataset(path_to_file, 1500)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uHTx_fYppKry",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "7f6f7fb1-3775-4314-b6ed-f5559950b75e"
      },
      "source": [
        "pairs = np.array(pairs)\n",
        "pairs[:5]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([['<start> go <end>', '<start> va <end>'],\n",
              "       ['<start> hi <end>', '<start> salut <end>'],\n",
              "       ['<start> run <end>', '<start> cours <end>'],\n",
              "       ['<start> run <end>', '<start> courez <end>'],\n",
              "       ['<start> who <end>', '<start> qui <end>']], dtype='<U45')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5h5xHF9BuU_V",
        "colab_type": "text"
      },
      "source": [
        "## WORD VECTORIZING:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hhpzD9_swct",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def word_vec(pairs):\n",
        "  english = pairs.T[0]\n",
        "  french = pairs.T[1]\n",
        "  vocab_english = []\n",
        "  vocab_french = []\n",
        "  for e, f in tqdm(zip(english, french), total=len(english), desc='Generating a Vocabulary'):\n",
        "    for w in e.split():\n",
        "      if w not in vocab_english:\n",
        "        vocab_english.append(w)\n",
        "    for w in f.split():\n",
        "      if w not in vocab_french:\n",
        "        vocab_french.append(w)\n",
        "  \n",
        "  english_word_embed = np.zeros((len(vocab_english), len(vocab_english)))\n",
        "  french_word_embed = np.zeros((len(vocab_french), len(vocab_french)))\n",
        "\n",
        "  for e, f in tqdm(zip(english, french), total=len(english), desc='Generating the Word Embeddings'):\n",
        "    e = e.split()\n",
        "    for i, w in enumerate(e[1:-1]):\n",
        "      i += 1\n",
        "      english_word_embed[vocab_english.index(w)][vocab_english.index(e[i-1])] += 1\n",
        "      english_word_embed[vocab_english.index(w)][vocab_english.index(e[i+1])] += 1\n",
        "    f = f.split()\n",
        "    for i, w in enumerate(f[1:-1]):\n",
        "      i += 1\n",
        "      french_word_embed[vocab_french.index(w)][vocab_french.index(f[i-1])] += 1\n",
        "      french_word_embed[vocab_french.index(w)][vocab_french.index(f[i+1])] += 1\n",
        "  return french_word_embed, english_word_embed, vocab_english, vocab_french"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNA3S3r_vJYn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "cbbdc97b-4d9c-4ce6-9611-2c1035dd896e"
      },
      "source": [
        "french_word_embed, english_word_embed, vocab_english, vocab_french = word_vec(pairs)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Generating a Vocabulary: 100%|██████████| 1500/1500 [00:00<00:00, 63322.36it/s]\n",
            "Generating the Word Embeddings: 100%|██████████| 1500/1500 [00:00<00:00, 15812.88it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8-3PZx6y2lX",
        "colab_type": "text"
      },
      "source": [
        "## TAKING A LOOK AT BOTH SPACES AND PREPARING TO MAP THEM:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qB8h_DJXy8T6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "1172b1d7-33a0-4752-ecfe-1adf951b7e6d"
      },
      "source": [
        "print(\"French\", french_word_embed.shape)\n",
        "print(\"English\", english_word_embed.shape)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "French (948, 948)\n",
            "English (512, 512)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dBaLuaIu14jL",
        "colab_type": "text"
      },
      "source": [
        "## SENTENCE TO VECTOR:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJtfm2aK18IH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sentence2vector(pairs, french_word_embed, english_word_embed, vocab_english, vocab_french):\n",
        "  english = pairs.T[0]\n",
        "  french = pairs.T[1]\n",
        "  sentences_english = []\n",
        "  sentences_french = []\n",
        "  for e, f in tqdm(zip(english, french), total=len(english), desc='Generating the Sentence Embeddings'):\n",
        "    eng = []\n",
        "    fra = []\n",
        "    e = e.split()[1:-1]\n",
        "    f = f.split()[1:-1]\n",
        "    for w in e:\n",
        "      eng.append(english_word_embed[vocab_english.index(w)])\n",
        "    for w in f:\n",
        "      fra.append(french_word_embed[vocab_french.index(w)])\n",
        "    eng = np.array(eng)\n",
        "    fra = np.array(fra)\n",
        "    eng = np.mean(eng, axis=0)\n",
        "    fra = np.mean(fra, axis=0)\n",
        "    sentences_english.append(eng)\n",
        "    sentences_french.append(fra)\n",
        "  sentences_english = np.array(sentences_english)\n",
        "  sentences_french = np.array(sentences_french)\n",
        "  return sentences_english, sentences_french    "
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-V5nJXH3Mb2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "51f7348c-98b8-436a-944e-3fdd4cad6dad"
      },
      "source": [
        "sentences_english, sentences_french = sentence2vector(pairs, french_word_embed, english_word_embed, vocab_english, vocab_french)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Generating the Sentence Embeddings: 100%|██████████| 1500/1500 [00:00<00:00, 15773.32it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clCYzdtu14RO",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gieuEY0PzEx4",
        "colab_type": "text"
      },
      "source": [
        "## MODEL FOR MAPPING:\n",
        "\n",
        "XR = Y"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_Jfd2XyvLeS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "                                    tf.keras.layers.Dense(256, activation='relu'),\n",
        "                                    tf.keras.layers.Dense(256, activation='relu'),\n",
        "                                    tf.keras.layers.Dense(512, activation='relu'),\n",
        "                                    tf.keras.layers.Dense(948, activation='relu'),\n",
        "])"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lOX4i-3jzR3V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='adam', loss='mse', metrics=['mae'])"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50lF_Ht8ztqi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 874
        },
        "outputId": "b4a25bf8-dd48-455b-93c1-573b4b922d30"
      },
      "source": [
        "model.fit(sentences_english, sentences_french, epochs=25)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "47/47 [==============================] - 0s 8ms/step - loss: 3.2818 - mae: 0.1467\n",
            "Epoch 2/25\n",
            "47/47 [==============================] - 0s 8ms/step - loss: 2.3220 - mae: 0.1191\n",
            "Epoch 3/25\n",
            "47/47 [==============================] - 0s 7ms/step - loss: 2.1835 - mae: 0.1154\n",
            "Epoch 4/25\n",
            "47/47 [==============================] - 0s 7ms/step - loss: 2.1423 - mae: 0.1146\n",
            "Epoch 5/25\n",
            "47/47 [==============================] - 0s 8ms/step - loss: 2.0731 - mae: 0.1129\n",
            "Epoch 6/25\n",
            "47/47 [==============================] - 0s 8ms/step - loss: 2.0444 - mae: 0.1121\n",
            "Epoch 7/25\n",
            "47/47 [==============================] - 0s 7ms/step - loss: 2.0115 - mae: 0.1119\n",
            "Epoch 8/25\n",
            "47/47 [==============================] - 0s 8ms/step - loss: 1.9673 - mae: 0.1103\n",
            "Epoch 9/25\n",
            "47/47 [==============================] - 0s 8ms/step - loss: 2.0389 - mae: 0.1116\n",
            "Epoch 10/25\n",
            "47/47 [==============================] - 0s 7ms/step - loss: 2.0993 - mae: 0.1126\n",
            "Epoch 11/25\n",
            "47/47 [==============================] - 0s 8ms/step - loss: 1.9460 - mae: 0.1097\n",
            "Epoch 12/25\n",
            "47/47 [==============================] - 0s 8ms/step - loss: 1.9642 - mae: 0.1102\n",
            "Epoch 13/25\n",
            "47/47 [==============================] - 0s 8ms/step - loss: 1.9530 - mae: 0.1098\n",
            "Epoch 14/25\n",
            "47/47 [==============================] - 0s 8ms/step - loss: 1.9528 - mae: 0.1093\n",
            "Epoch 15/25\n",
            "47/47 [==============================] - 0s 8ms/step - loss: 1.9248 - mae: 0.1087\n",
            "Epoch 16/25\n",
            "47/47 [==============================] - 0s 8ms/step - loss: 2.0154 - mae: 0.1098\n",
            "Epoch 17/25\n",
            "47/47 [==============================] - 0s 7ms/step - loss: 1.9529 - mae: 0.1093\n",
            "Epoch 18/25\n",
            "47/47 [==============================] - 0s 8ms/step - loss: 1.9116 - mae: 0.1084\n",
            "Epoch 19/25\n",
            "47/47 [==============================] - 0s 8ms/step - loss: 1.8719 - mae: 0.1078\n",
            "Epoch 20/25\n",
            "47/47 [==============================] - 0s 8ms/step - loss: 1.9091 - mae: 0.1079\n",
            "Epoch 21/25\n",
            "47/47 [==============================] - 0s 7ms/step - loss: 1.9234 - mae: 0.1081\n",
            "Epoch 22/25\n",
            "47/47 [==============================] - 0s 8ms/step - loss: 1.9044 - mae: 0.1077\n",
            "Epoch 23/25\n",
            "47/47 [==============================] - 0s 8ms/step - loss: 1.8946 - mae: 0.1075\n",
            "Epoch 24/25\n",
            "47/47 [==============================] - 0s 8ms/step - loss: 1.9290 - mae: 0.1076\n",
            "Epoch 25/25\n",
            "47/47 [==============================] - 0s 8ms/step - loss: 1.9672 - mae: 0.1085\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fb88ecc8a90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LWuZ6_80KoB",
        "colab_type": "text"
      },
      "source": [
        "## HUMAN TESTING:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MwO24F3Vz8vP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word = \"hi\"\n",
        "index_word = vocab_english.index(word)\n",
        "english_vector = np.array([english_word_embed[index_word]])\n",
        "predicted_french_vector = model.predict(english_vector)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S9z04_7y0qua",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ca1f3aa6-49c7-4bd5-9af3-3b11bddfdd50"
      },
      "source": [
        "neigh = KNeighborsClassifier(n_neighbors=1)\n",
        "neigh.fit(french_word_embed, vocab_french)\n",
        "print(neigh.predict(predicted_french_vector))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['fantastique']\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}