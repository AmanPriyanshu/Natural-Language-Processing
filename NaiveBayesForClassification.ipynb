{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NaiveBayesForClassification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOVnoVq39VeDc5DPZilAKru",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AmanPriyanshu/Natural-Language-Processing/blob/master/NaiveBayesForClassification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhibqTTimGQq",
        "colab_type": "text"
      },
      "source": [
        "## Downloading Dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQXMdlq-l0Sd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "bc7ce79e-f217-45ca-b5a9-1711b50a9d72"
      },
      "source": [
        "!mkdir -p data\n",
        "!wget -nc https://nyc3.digitaloceanspaces.com/ml-files-distro/v1/sentiment-analysis-is-bad/data/sentiment140-subset.csv.zip -P data\n",
        "!unzip -n -d data data/sentiment140-subset.csv.zip"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "File ‘data/sentiment140-subset.csv.zip’ already there; not retrieving.\n",
            "\n",
            "Archive:  data/sentiment140-subset.csv.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VH0L2y__mKzI",
        "colab_type": "text"
      },
      "source": [
        "## IMPORTS:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3UozNNemKla",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "1d935b22-bdf5-4197-d240-71bc2a87ad8b"
      },
      "source": [
        "import nltk\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "from tqdm import tqdm\n",
        "import string\n",
        "import tensorflow as tf"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ET6ZRIDm8ep",
        "colab_type": "text"
      },
      "source": [
        "### Only importing 30,000 values since this is more of practice and demo for refrence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3s7f_Y7mCsh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "outputId": "0bef8f8b-76d8-4bef-c6a6-6652c888a089"
      },
      "source": [
        "df = pd.read_csv(\"data/sentiment140-subset.csv\", nrows=30000)\n",
        "print(df.head())"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   polarity                                               text\n",
            "0         0                      @kconsidder You never tweet  \n",
            "1         0                 Sick today  coding from the couch.\n",
            "2         1  @ChargerJenn Thx for answering so quick,I was ...\n",
            "3         1  Wii fit says I've lost 10 pounds since last ti...\n",
            "4         0  @MrKinetik Not a thing!!!  I don't really have...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6GH6fL3nCPP",
        "colab_type": "text"
      },
      "source": [
        "### Let's count positives and negatives"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-5U2Nrk-m_Tv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "708052a8-61b4-4b7b-e396-085e6b6866f4"
      },
      "source": [
        "df.polarity.value_counts()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    15064\n",
              "0    14936\n",
              "Name: polarity, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFfbtn3unFKv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "c01c8f0b-8f4b-412b-9042-98977aa1fe36"
      },
      "source": [
        "df = df.values\n",
        "print(df)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0 '@kconsidder You never tweet  ']\n",
            " [0 'Sick today  coding from the couch.']\n",
            " [1\n",
            "  '@ChargerJenn Thx for answering so quick,I was afraid I was gonna crash twitter with all the spamming I did 2 RR..sorry bout that ']\n",
            " ...\n",
            " [1\n",
            "  '@phnompenhpost thanks for the follow! u guys do a great job in reporting news about Cambodia...makes me proud to be cambodian ']\n",
            " [0\n",
            "  \"@coliwilso crapï¿½ I really wanted to make it for @minmï¿½ but I'm feeling way too tired after the whole weekend \"]\n",
            " [1\n",
            "  'follow friday- @theclassiccrime @jeremycamp @chris_daughtry &amp; @dannygokey ']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WaMYJm7MnHmZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "polarity = df.T[0].flatten()\n",
        "tweets = df.T[1].flatten()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_8K4VlXnJmq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "outputId": "348c95d8-ec91-40ce-d6a9-551677131418"
      },
      "source": [
        "tweets"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['@kconsidder You never tweet  ',\n",
              "       'Sick today  coding from the couch.',\n",
              "       '@ChargerJenn Thx for answering so quick,I was afraid I was gonna crash twitter with all the spamming I did 2 RR..sorry bout that ',\n",
              "       ...,\n",
              "       '@phnompenhpost thanks for the follow! u guys do a great job in reporting news about Cambodia...makes me proud to be cambodian ',\n",
              "       \"@coliwilso crapï¿½ I really wanted to make it for @minmï¿½ but I'm feeling way too tired after the whole weekend \",\n",
              "       'follow friday- @theclassiccrime @jeremycamp @chris_daughtry &amp; @dannygokey '],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXDd8tYWnMVu",
        "colab_type": "text"
      },
      "source": [
        "## PREPROCESSING:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CCPAJLdnnLsD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def stopwords_punctuation(arr):\n",
        "  new_arr = []\n",
        "  diction = {}\n",
        "  for p in string.punctuation:\n",
        "    diction.update({p:' '})\n",
        "  for s in tqdm(arr):\n",
        "    s = s.translate(str.maketrans(diction))\n",
        "    new_arr.append(' '.join([i for i in s.split() if i not in stopwords.words('english')]))\n",
        "  new_arr = np.array(new_arr)\n",
        "  return new_arr"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Ftuq7KdnRWa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def stemming_lowercase(arr):\n",
        "  porter = PorterStemmer()\n",
        "  stemmed_arr = []\n",
        "  for s in tqdm(arr):\n",
        "    s = s.lower()\n",
        "    stemmed_arr.append(' '.join([porter.stem(word) for word in s.split()]))\n",
        "  stemmed_arr = np.array(stemmed_arr)\n",
        "  return stemmed_arr"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FHL1nDZSnTmx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "bce46dd7-df2b-4986-d6cd-4a0ecb93806b"
      },
      "source": [
        "tweets = stopwords_punctuation(tweets)\n",
        "tweets = stemming_lowercase(tweets)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 30000/30000 [00:46<00:00, 650.61it/s]\n",
            "100%|██████████| 30000/30000 [00:04<00:00, 6260.32it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQv1NVsNnjtD",
        "colab_type": "text"
      },
      "source": [
        "## TRAINING:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QdwZ2MpZnWNo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def word_freq(polarity_arr, str_arr):\n",
        "  word_counts = {}\n",
        "  total = [0, 0]\n",
        "  logprior = [0, 0]\n",
        "  for p, s in tqdm(zip(polarity_arr, str_arr), total=len(str_arr)):\n",
        "    logprior[p] += 1\n",
        "    s = s.split()\n",
        "    for w in s:\n",
        "      if w not in list(word_counts.keys()):\n",
        "        word_counts.update({w:[1, 1]})\n",
        "        total[0] += 1\n",
        "        total[1] += 1\n",
        "      word_counts[w][p] += 1\n",
        "      total[p] += 1\n",
        "  total = np.array(total)\n",
        "  for w,f in word_counts.items():\n",
        "    word_counts[w] = np.array(f)/total\n",
        "  lambda_words = {} \n",
        "  for w,f in word_counts.items():\n",
        "    lambda_words.update({w: np.log(f[1]/f[0])})\n",
        "  logprior = np.array(logprior)\n",
        "  logprior = logprior[1]/logprior[0]\n",
        "  return lambda_words, logprior"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rjMK5v8spT1O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b8b4e4d6-7807-45e9-986e-df11986f747f"
      },
      "source": [
        "lambda_words, logprior = word_freq(polarity[:int(0.5*len(tweets))], tweets[:int(0.5*len(tweets))])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 15000/15000 [00:20<00:00, 719.69it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wsmeNiIarbQV",
        "colab_type": "text"
      },
      "source": [
        "## TESTING:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNGRhOe8prnK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def testing(arr, lambda_words, logprior):\n",
        "  y_pred = []\n",
        "  for s in tqdm(arr):\n",
        "    s = s.split()\n",
        "    s = list(set(s))\n",
        "    l = logprior\n",
        "    for w in s:\n",
        "      try:\n",
        "        l += lambda_words[w]\n",
        "      except:\n",
        "        pass\n",
        "    if l>0:\n",
        "      y_pred.append(1)\n",
        "    else:\n",
        "      y_pred.append(0)\n",
        "  y_pred = np.array(y_pred)\n",
        "  return y_pred"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rp9c5miMrci5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "f4522592-19ac-48fc-a1cd-0e5743173192"
      },
      "source": [
        "y_pred = testing(tweets, lambda_words, logprior)\n",
        "accuracy = 1 - np.mean(np.abs(y_pred - polarity))\n",
        "print('\\n\\nAccuracy',accuracy)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 30000/30000 [00:00<00:00, 156481.50it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Accuracy 0.7925\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}